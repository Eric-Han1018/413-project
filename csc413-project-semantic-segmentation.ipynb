{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":34686,"sourceType":"datasetVersion","datasetId":27201},{"sourceId":7659506,"sourceType":"datasetVersion","datasetId":4465982}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"papermill":{"default_parameters":{},"duration":1947.203929,"end_time":"2024-04-08T23:35:24.495173","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-04-08T23:02:57.291244","version":"2.3.3"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Data preprocessing","metadata":{"papermill":{"duration":0.041506,"end_time":"2024-04-08T23:03:04.570154","exception":false,"start_time":"2024-04-08T23:03:04.528648","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import imageio\nimport random\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport glob\nimport cv2\nimport tensorflow as tf\nimport tensorflow.keras.layers as tfl\n\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.layers import Activation, Input, Conv2D, MaxPooling2D, BatchNormalization, Conv2DTranspose, concatenate, Concatenate, UpSampling2D, AveragePooling2D\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.applications import ResNet50\nfrom sklearn.model_selection import train_test_split\nimport tensorflow.keras as keras\n\n\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"papermill":{"duration":5.120708,"end_time":"2024-04-08T23:03:09.729466","exception":false,"start_time":"2024-04-08T23:03:04.608758","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-15T01:10:49.023674Z","iopub.execute_input":"2024-04-15T01:10:49.024027Z","iopub.status.idle":"2024-04-15T01:10:49.031654Z","shell.execute_reply.started":"2024-04-15T01:10:49.023998Z","shell.execute_reply":"2024-04-15T01:10:49.030581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images_list = []\nmasks_list = []\ndef store_images(data):\n    image = glob.glob('/kaggle/input/lyft-udacity-challenge/'+data+'/'+data+'/CameraRGB/*.png')\n    mask = glob.glob('/kaggle/input/lyft-udacity-challenge/'+data+'/'+data+'/CameraSeg/*.png')\n    images_list.extend(image)\n    masks_list.extend(mask)   \nfor data in ['dataA', 'dataB', 'dataC', 'dataD', 'dataE']:\n    store_images(data)\n\ndataset = tf.data.Dataset.from_tensor_slices((images_list, masks_list))\ndataset_length = sum(1 for _ in dataset)\n\nprint(\"Length of dataset:\", dataset_length)\n# for image, mask in dataset.take(1):\n#     print(\"Image File:\", image.numpy().decode(), \"Mask File:\", mask.numpy().decode())","metadata":{"execution":{"iopub.status.busy":"2024-04-15T01:10:52.334544Z","iopub.execute_input":"2024-04-15T01:10:52.334905Z","iopub.status.idle":"2024-04-15T01:10:56.672444Z","shell.execute_reply.started":"2024-04-15T01:10:52.334877Z","shell.execute_reply":"2024-04-15T01:10:56.67149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_images(data, num):\n    # load images\n    images = glob.glob('/kaggle/input/lyft-udacity-challenge/'+data+'/'+data+'/CameraRGB/*.png')[:num]\n    masks = glob.glob('/kaggle/input/lyft-udacity-challenge/'+data+'/'+data+'/CameraSeg/*.png')[:num]\n\n    for i in range(len(images)):\n        images[i] = cv2.imread(images[i])\n        masks[i] = cv2.imread(masks[i], cv2.IMREAD_GRAYSCALE)\n\n    fig, axes = plt.subplots(num, 2)\n\n    for i in range(len(images)):\n        axes[i][0].imshow(cv2.cvtColor(images[i], cv2.COLOR_BGR2RGB))\n        axes[i][1].imshow(masks[i])\n        axes[i][0].axis('off')\n        axes[i][1].axis('off')\n    plt.show()\n\nshow_images('dataB', 2)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-15T01:11:21.815732Z","iopub.execute_input":"2024-04-15T01:11:21.816063Z","iopub.status.idle":"2024-04-15T01:11:22.335631Z","shell.execute_reply.started":"2024-04-15T01:11:21.81604Z","shell.execute_reply":"2024-04-15T01:11:22.334745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_datas(image_path, mask_path):\n    image = tf.io.read_file(image_path)\n    image = tf.image.decode_png(image, channels=3)\n    image = tf.image.convert_image_dtype(image, tf.float32)\n\n    mask = tf.io.read_file(mask_path)\n    mask = tf.image.decode_png(mask, channels=3)\n    mask = tf.math.reduce_max(mask, axis=-1, keepdims=True)\n    return image, mask\n\ndef preprocess(image, mask, height=256, width=256):\n    input_image = tf.image.resize(image, (height, width), method='nearest')\n    input_mask = tf.image.resize(mask, (height, width), method='nearest')\n\n    return input_image, input_mask\n\ndataset_load = dataset.map(load_datas)\ndataset_processed = dataset_load.map(preprocess)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T01:11:30.06133Z","iopub.execute_input":"2024-04-15T01:11:30.062023Z","iopub.status.idle":"2024-04-15T01:11:30.421314Z","shell.execute_reply.started":"2024-04-15T01:11:30.061988Z","shell.execute_reply":"2024-04-15T01:11:30.420579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 15\nBATCH_SIZE = 32","metadata":{"execution":{"iopub.status.busy":"2024-04-15T01:11:33.999185Z","iopub.execute_input":"2024-04-15T01:11:33.999628Z","iopub.status.idle":"2024-04-15T01:11:34.00383Z","shell.execute_reply.started":"2024-04-15T01:11:33.999599Z","shell.execute_reply":"2024-04-15T01:11:34.002835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def split_dataset(data, train_split=0.7, val_split=0.15, test_split=0.15, shuffle=True, shuffle_size=1000, batch_size=BATCH_SIZE):\n    \"\"\"\n    Splits the dataset into training, validation, and test sets.\n    Returns: (train_dataset, val_dataset, test_dataset).\n    \"\"\"\n\n    assert train_split + val_split + test_split == 1, \"Split ratios must sum to 1\"\n\n    # Determine the size of the dataset\n    dataset_size = len(list(data))\n\n    # Calculate split sizes\n    train_size = int(train_split * dataset_size)\n    val_size = int(val_split * dataset_size)\n\n    # Shuffle the dataset\n    if shuffle:\n        data = data.shuffle(shuffle_size, reshuffle_each_iteration=False)\n\n    # Split the dataset\n    train_dataset = data.take(train_size)\n    val_dataset = data.skip(train_size).take(val_size)\n    test_dataset = data.skip(train_size + val_size)\n\n    # Batch the datasets\n    train_dataset = train_dataset.batch(batch_size)\n    val_dataset = val_dataset.batch(batch_size)\n    test_dataset = test_dataset.batch(batch_size)\n\n    return train_dataset, val_dataset, test_dataset\n\ntrain_dataset, val_dataset, test_dataset = split_dataset(dataset_processed)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T01:11:37.152922Z","iopub.execute_input":"2024-04-15T01:11:37.153625Z","iopub.status.idle":"2024-04-15T01:12:35.698568Z","shell.execute_reply.started":"2024-04-15T01:11:37.153594Z","shell.execute_reply":"2024-04-15T01:12:35.697773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  2. Model Architectures\nIn this project, 3 model architectures are built, trained and compared, to see each performance on semantic segmentation tasks for autonomous driving.\n\n### 1. Fully Convolutional Network (FCN)\nFCN is a deep learning architecture for semantic segmentation tasks, retaining spatial information with convolutional layers instead of fully connected layers.\n\n### 2. U-Net\nU-Net is a CNN architecture initally used for biomedical image segmentation, featuring a contracting and expansive path with skip connections for precise localization.\n\n### 3. DeepLabV3\nDeepLabV3, developed by Google Research, is a cutting-edge architecture for semantic image segmentation tasks.\n","metadata":{}},{"cell_type":"markdown","source":"* ###  Model architecture for FCN","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, ReLU, Conv2DTranspose, concatenate, Activation\n\ndef conv_block_3(inputs=None, n_filters=32, dropout_prob=0):\n    layer = Conv2D(n_filters, 3, padding='same', kernel_initializer='he_normal')(inputs)\n    layer = BatchNormalization(axis=3)(layer)\n    layer = Activation(\"relu\")(layer)\n    layer = Conv2D(n_filters, 3, padding='same', kernel_initializer='he_normal')(layer)\n    layer = BatchNormalization(axis=3)(layer)\n    layer = Activation(\"relu\")(layer)\n\n    if dropout_prob > 0:\n        layer = tf.keras.layers.Dropout(dropout_prob)(layer)\n\n    return layer\n\ndef decoder_block(input_tensor, concat_tensor, n_filters):\n    # Upsample the input\n    decoder = Conv2DTranspose(n_filters, (3, 3), strides=(1, 1), padding='same')(input_tensor)\n    # Concatenate with the corresponding layer from the encoder\n    decoder = concatenate([decoder, concat_tensor], axis=-1)\n    # Additional convolutional layers\n    decoder = Conv2D(n_filters, (3, 3), padding='same')(decoder)\n    decoder = BatchNormalization()(decoder)\n    decoder = Activation('relu')(decoder)\n\n    return decoder\n\ndef fcn_model(input_size=(256, 256, 3), n_filters=32, n_classes=13):\n    inputs = Input(input_size)\n\n    # Encoder\n    cblock1 = conv_block_3(inputs, n_filters) # (None, 256, 256, 32)\n    cblock2 = conv_block_3(cblock1, n_filters * 2) # (None, 256, 256, 64)\n    cblock3 = conv_block_3(cblock2, n_filters * 4) # (None, 256, 256, 128)\n\n    # Decoder\n    dblock1 = decoder_block(cblock3, cblock2, n_filters * 2) # (None, 256, 256, 64)\n    dblock2 = decoder_block(dblock1, cblock1, n_filters) # (None, 256, 256, 32)\n\n    # Output layer: (None, 256, 256, 13)\n    layer = Conv2D(n_classes, 3, padding='same', kernel_initializer='he_normal')(cblock3)\n    layer = BatchNormalization(axis=3)(layer)\n\n    model = tf.keras.Model(inputs=inputs, outputs=layer)\n    model.compile(optimizer=tf.keras.optimizers.Adam(),\n                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n                  metrics=['accuracy'])\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-04-15T01:16:39.315398Z","iopub.execute_input":"2024-04-15T01:16:39.316099Z","iopub.status.idle":"2024-04-15T01:16:39.328581Z","shell.execute_reply.started":"2024-04-15T01:16:39.316067Z","shell.execute_reply":"2024-04-15T01:16:39.327698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* ###  Model architecture for U-Net","metadata":{}},{"cell_type":"code","source":"def conv_layer(inputs, n_filters, kernel_size=3):\n    \"\"\"\n    Apply convolution, batch normalization, and ReLU activation.\n\n    Args:\n    - inputs: Input tensor.\n    - n_filters: Number of filters for the convolutional layer.\n    - kernel_size: Size of the convolution kernel.\n\n    Returns:\n    - Output tensor after the operations.\n    \"\"\"\n    layer = tfl.Conv2D(n_filters, kernel_size=kernel_size, padding='same', kernel_initializer='he_normal')(inputs)\n    layer = tfl.BatchNormalization(axis=3)(layer)\n    layer = tfl.ReLU()(layer)\n    return layer","metadata":{"execution":{"iopub.status.busy":"2024-04-15T01:16:43.214845Z","iopub.execute_input":"2024-04-15T01:16:43.215669Z","iopub.status.idle":"2024-04-15T01:16:43.221111Z","shell.execute_reply.started":"2024-04-15T01:16:43.215637Z","shell.execute_reply":"2024-04-15T01:16:43.220175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def conv_block(inputs, n_filters, dropout_prob=0, max_pooling=True):\n    \"\"\"\n    Perform two convolutions with optional dropout and max pooling.\n\n    Args:\n    - inputs: Input tensor.\n    - n_filters: Number of filters for the convolutional layers.\n    - dropout_prob: Dropout rate.\n    - max_pooling: Boolean, whether to include a max pooling layer.\n\n    Returns:\n    - next_layer: Output tensor for the next layer.\n    - skip_connection: Output tensor for the skip connection.\n    \"\"\"\n    # Two convolutional layers\n    layer = conv_layer(inputs, n_filters)\n    layer = conv_layer(layer, n_filters)\n\n    # Optional dropout\n    if dropout_prob > 0:\n        layer = tfl.Dropout(dropout_prob)(layer)\n    # Optional max pooling\n    if max_pooling:\n        next_layer = tfl.MaxPooling2D(pool_size=(2, 2))(layer)\n    else:\n        next_layer = layer\n\n    skip_connection = layer\n\n    return next_layer, skip_connection","metadata":{"execution":{"iopub.status.busy":"2024-04-15T01:16:46.158795Z","iopub.execute_input":"2024-04-15T01:16:46.159662Z","iopub.status.idle":"2024-04-15T01:16:46.166564Z","shell.execute_reply.started":"2024-04-15T01:16:46.15963Z","shell.execute_reply":"2024-04-15T01:16:46.165603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def upsampling_block(inputs, skip_connection_inputs, n_filters):\n    \"\"\"\n    Upsample the input and merge with the skip connection.\n\n    Args:\n    - inputs: Input tensor from the previous layer.\n    - skip_connection_inputs: Input tensor from the corresponding contraction block (for the skip connection).\n    - n_filters: Number of filters for the convolutional layers.\n\n    Returns:\n    - layer: Output tensor after upsampling and convolution.\n    \"\"\"\n    # Upsampling\n    up = tfl.Conv2DTranspose(n_filters, kernel_size=3, strides=(2, 2), padding='same')(inputs)\n    # Merging with skip connection\n    merge = tfl.concatenate([up, skip_connection_inputs], axis=3)\n    # Two convolutional layers\n    layer = conv_layer(merge, n_filters)\n    layer = conv_layer(layer, n_filters)\n\n    return layer","metadata":{"execution":{"iopub.status.busy":"2024-04-14T15:45:30.165736Z","iopub.execute_input":"2024-04-14T15:45:30.166052Z","iopub.status.idle":"2024-04-14T15:45:30.172391Z","shell.execute_reply.started":"2024-04-14T15:45:30.166024Z","shell.execute_reply":"2024-04-14T15:45:30.171575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# U-Net model\ndef unet_model(input_size=(256, 256, 3), n_filters=32, n_classes=13):\n    \"\"\"\n    Define the U-Net model architecture.\n\n    Args:\n    - input_size: Shape of the input images.\n    - n_filters: Number of filters for the convolutional layers in the first block. Gets doubled in each subsequent block.\n    - n_classes: Number of output classes.\n\n    Returns:\n    - model: Compiled U-Net model.\n    \"\"\"\n    inputs = tfl.Input(input_size)\n\n    # Encoding path\n    cblock1 = conv_block(inputs, n_filters)\n    cblock2 = conv_block(cblock1[0], n_filters * 2)\n    cblock3 = conv_block(cblock2[0], n_filters * 4)\n    cblock4 = conv_block(cblock3[0], n_filters * 8, dropout_prob=0.3)\n    cblock5 = conv_block(cblock4[0], n_filters * 16, dropout_prob=0.3, max_pooling=False)\n\n    # Decoding path\n    ublock6 = upsampling_block(cblock5[0], cblock4[1], n_filters * 8)\n    ublock7 = upsampling_block(ublock6, cblock3[1], n_filters * 4)\n    ublock8 = upsampling_block(ublock7, cblock2[1], n_filters * 2)\n    ublock9 = upsampling_block(ublock8, cblock1[1], n_filters)\n\n    # Output layer\n    output_layer = conv_layer(ublock9, n_filters)\n    output_layer = tfl.Conv2D(n_classes, kernel_size=1, padding='same')(output_layer)\n\n    model = tf.keras.Model(inputs=inputs, outputs=output_layer)\n    model.compile(optimizer=tf.keras.optimizers.Adam(),\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-04-14T15:45:30.173567Z","iopub.execute_input":"2024-04-14T15:45:30.173913Z","iopub.status.idle":"2024-04-14T15:45:30.185995Z","shell.execute_reply.started":"2024-04-14T15:45:30.173871Z","shell.execute_reply":"2024-04-14T15:45:30.185015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* ###  Model architecture for DeepLabV3","metadata":{}},{"cell_type":"code","source":"def aspp(input):\n    shape = input.shape\n    y_pool = AveragePooling2D(pool_size = (shape[1], shape[2]))(input)\n    y_pool = Conv2D(filters=256, padding='same', use_bias=False, kernel_size=1)(y_pool)\n    y_pool = BatchNormalization()(y_pool)\n    y_pool = Activation(\"relu\")(y_pool)\n    y_pool = UpSampling2D((shape[1], shape[2]), interpolation=\"bilinear\")(y_pool)\n\n    out_1 = Conv2D(filters=256, padding='same', dilation_rate = 1, use_bias=False, kernel_size=1)(input)\n    out_1 = BatchNormalization()(out_1)\n    out_1 = Activation(\"relu\")(out_1)\n\n    out_6 = Conv2D(filters=256, padding='same', dilation_rate = 6, use_bias=False, kernel_size=1)(input)\n    out_6 = BatchNormalization()(out_6)\n    out_6 = Activation(\"relu\")(out_6)\n\n    out_12 = Conv2D(filters=256, padding='same', dilation_rate = 12, use_bias=False, kernel_size=1)(input)\n    out_12 = BatchNormalization()(out_12)\n    out_12 = Activation(\"relu\")(out_12)\n\n    out_18 = Conv2D(filters=256, padding='same', dilation_rate = 18, use_bias=False, kernel_size=1)(input)\n    out_18 = BatchNormalization()(out_18)\n    out_18 = Activation(\"relu\")(out_18)\n\n    y = Concatenate()([y_pool, out_1, out_6, out_12, out_18])\n    y = Conv2D(filters=256, padding='same', dilation_rate = 1, use_bias=False, kernel_size=1)(y)\n    y = BatchNormalization()(y)\n    y = Activation(\"relu\")(y)\n\n    return y\n\ndef DeeplabV3(image_size=(256, 256, 3), num_classes=13):\n\n    inputs = Input(image_size)\n\n    resnet50 = keras.applications.ResNet50(\n        weights=\"imagenet\", include_top=False, input_tensor=inputs\n    )\n\n    # pretrained Resnet50 output\n#     x1 = resnet50.get_layer(\"conv4_block6_out\").output\n    x1 = resnet50.get_layer(\"conv4_block6_2_relu\").output\n    x1 = aspp(x1)\n    x1 = UpSampling2D((4, 4), interpolation=\"bilinear\")(x1)\n\n    # low level features\n#     x2 = resnet50.get_layer(\"conv2_block2_out\").output\n    x2 = resnet50.get_layer(\"conv2_block3_2_relu\").output\n    x2 = Conv2D(filters=48, padding='same', use_bias=False, kernel_size=1)(x2)\n    x2 = BatchNormalization()(x2)\n    x2 = Activation(\"relu\")(x2)\n\n    x = Concatenate()([x1, x2])\n\n    x = Conv2D(filters=256, padding='same', activation='relu', use_bias=False, kernel_size=3)(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n\n    x = Conv2D(filters=256, padding='same', activation='relu', use_bias=False, kernel_size=3)(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n\n    x = UpSampling2D((4, 4), interpolation=\"bilinear\")(x)\n\n    # output\n    x = Conv2D(num_classes, kernel_size=(1, 1), name=\"output_layer\", padding=\"same\")(x)\n    x = Activation('sigmoid')(x)\n\n    model = Model(inputs=inputs, outputs=x)\n    \n    model.compile(optimizer=tf.keras.optimizers.Adam(),\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n    \n    return model\n","metadata":{"papermill":{"duration":2.344907,"end_time":"2024-04-08T23:03:43.353902","exception":false,"start_time":"2024-04-08T23:03:41.008995","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-14T15:45:30.19971Z","iopub.execute_input":"2024-04-14T15:45:30.199973Z","iopub.status.idle":"2024-04-14T15:45:30.217581Z","shell.execute_reply.started":"2024-04-14T15:45:30.199951Z","shell.execute_reply":"2024-04-14T15:45:30.216677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Model Training","metadata":{"papermill":{"duration":0.093809,"end_time":"2024-04-08T23:03:43.740154","exception":false,"start_time":"2024-04-08T23:03:43.646345","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### Hyperparameter toning and model configerations. ","metadata":{}},{"cell_type":"code","source":"img_height = 256\nimg_width = 256\nnum_channels = 3\nfilters = 32\nn_classes = 13","metadata":{"execution":{"iopub.status.busy":"2024-04-15T01:16:59.791872Z","iopub.execute_input":"2024-04-15T01:16:59.792282Z","iopub.status.idle":"2024-04-15T01:16:59.797019Z","shell.execute_reply.started":"2024-04-15T01:16:59.792254Z","shell.execute_reply":"2024-04-15T01:16:59.795974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Function to plot training history","metadata":{}},{"cell_type":"code","source":"def plot_training_history(history):\n    acc = [0.] + history.history.get('accuracy', [])\n    val_acc = [0.] + history.history.get('val_accuracy', [])\n    loss = history.history.get('loss', [])\n    val_loss = history.history.get('val_loss', [])\n\n    # Plotting\n    plt.figure(figsize=(8, 8))\n\n    # Accuracy subplot\n    plt.subplot(2, 1, 1)\n    plt.plot(acc, label='Training Accuracy')\n    plt.plot(val_acc, label='Validation Accuracy')\n    plt.legend(loc='lower right')\n    plt.ylabel('Accuracy')\n    plt.title('Training and Validation Accuracy')\n    plt.ylim([0, 1])\n\n    # Loss subplot\n    plt.subplot(2, 1, 2)\n    plt.plot(loss, label='Training Loss')\n    plt.plot(val_loss, label='Validation Loss')\n    plt.legend(loc='upper right')\n    plt.ylabel('Cross Entropy')\n    plt.title('Training and Validation Loss')\n    plt.xlabel('Epoch')\n    plt.ylim([0, 1])\n\n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-15T01:17:02.346577Z","iopub.execute_input":"2024-04-15T01:17:02.347203Z","iopub.status.idle":"2024-04-15T01:17:02.355898Z","shell.execute_reply.started":"2024-04-15T01:17:02.347168Z","shell.execute_reply":"2024-04-15T01:17:02.35487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* ### Training FCN","metadata":{}},{"cell_type":"code","source":"import time\n\n# FCN model\n\nfcn_model1 = fcn_model()\nfcn_model1.summary()","metadata":{"execution":{"iopub.status.busy":"2024-04-15T01:17:07.24712Z","iopub.execute_input":"2024-04-15T01:17:07.248186Z","iopub.status.idle":"2024-04-15T01:17:07.483286Z","shell.execute_reply.started":"2024-04-15T01:17:07.248152Z","shell.execute_reply":"2024-04-15T01:17:07.482417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_time = time.time()\n\n# reduce_lr = ReduceLROnPlateau(monitor= \"loss\", factor=0.1,\n#                               patience= 1, min_lr= 1e-6)\n# early_stop = EarlyStopping(patience= 2)\n\nreduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.1,\n                              patience=10, min_lr=1e-6)\nearly_stop = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n\nfcn_result1 = fcn_model1.fit(train_dataset,\n                    validation_data=val_dataset,\n                    epochs=EPOCHS,\n                    callbacks=[reduce_lr,early_stop],\n                    batch_size=BATCH_SIZE)\nend_time = time.time()\ntraining_time = end_time - start_time","metadata":{"execution":{"iopub.status.busy":"2024-04-15T01:17:20.351563Z","iopub.execute_input":"2024-04-15T01:17:20.352234Z","iopub.status.idle":"2024-04-15T01:21:00.708815Z","shell.execute_reply.started":"2024-04-15T01:17:20.352203Z","shell.execute_reply":"2024-04-15T01:21:00.707865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"----------------------------------------------------------------\")\nprint(\"Training time for FCN:\", training_time, \"seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-04-15T01:21:29.576433Z","iopub.execute_input":"2024-04-15T01:21:29.577106Z","iopub.status.idle":"2024-04-15T01:21:29.581804Z","shell.execute_reply.started":"2024-04-15T01:21:29.577074Z","shell.execute_reply":"2024-04-15T01:21:29.580904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_training_history(fcn_result1)","metadata":{"execution":{"iopub.status.busy":"2024-04-14T15:48:18.166188Z","iopub.execute_input":"2024-04-14T15:48:18.166488Z","iopub.status.idle":"2024-04-14T15:48:18.686654Z","shell.execute_reply.started":"2024-04-14T15:48:18.166464Z","shell.execute_reply":"2024-04-14T15:48:18.685704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* ###  Training U-Net","metadata":{}},{"cell_type":"code","source":"# U-net model\nunet_model1 = unet_model()\nunet_model1.summary()","metadata":{"execution":{"iopub.status.busy":"2024-04-14T15:48:18.688091Z","iopub.execute_input":"2024-04-14T15:48:18.688965Z","iopub.status.idle":"2024-04-14T15:48:19.039247Z","shell.execute_reply.started":"2024-04-14T15:48:18.688928Z","shell.execute_reply":"2024-04-14T15:48:19.038466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_time = time.time()\n\nunet_model1 = unet_model()\nreduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.1,\n                              patience=10, min_lr=1e-6)\nearly_stop = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n\nunet_result1 = unet_model1.fit(train_dataset,\n                    validation_data=val_dataset,\n                    epochs=EPOCHS,\n                    callbacks=[reduce_lr,early_stop],\n                    batch_size=BATCH_SIZE)\nend_time = time.time()","metadata":{"execution":{"iopub.status.busy":"2024-04-14T15:48:19.040455Z","iopub.execute_input":"2024-04-14T15:48:19.041128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_time = end_time - start_time\nprint(\"----------------------------------------------------------------\")\nprint(\"Training time for U-Net:\", training_time, \"seconds\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_training_history(unet_result1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* ###  Training DeepLabV3","metadata":{}},{"cell_type":"code","source":"# Deeplab model\ndeeplab_model1 = DeeplabV3()\ndeeplab_model1.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_time = time.time()\n\nearly_stop = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\nreduce_lr = ReduceLROnPlateau(monitor='val_accuracy',factor=1e-1, patience=10, verbose=1, min_lr = 1e-6)\n\ndeeplab_result1 = deeplab_model1.fit(train_dataset, \n                    validation_data = val_dataset, \n                    epochs = EPOCHS, \n                    callbacks=[early_stop, reduce_lr], \n                    batch_size = BATCH_SIZE)\nend_time = time.time()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_time = end_time - start_time\nprint(\"----------------------------------------------------------------\")\nprint(\"Training time for Deeplab:\", training_time, \"seconds\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_training_history(deeplab_result1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Evaluation","metadata":{}},{"cell_type":"markdown","source":"* ### Accuracy","metadata":{}},{"cell_type":"code","source":"def model_accuracy(model):\n    train_loss, train_accuracy = model.evaluate(train_dataset, batch_size = BATCH_SIZE)\n    validation_loss, validation_accuracy = model.evaluate(val_dataset, batch_size = BATCH_SIZE)\n    test_loss, test_accuracy = model.evaluate(test_dataset, batch_size = BATCH_SIZE)\n    print(f'Model Accuracy on the Training Dataset: {round(train_accuracy * 100, 2)}%')\n    print(f'Model Accuracy on the Validation Dataset: {round(validation_accuracy * 100, 2)}%')\n    print(f'Model Accuracy on the Test Dataset: {round(test_accuracy * 100, 2)}%')","metadata":{"papermill":{"duration":1.447569,"end_time":"2024-04-08T23:31:48.982079","exception":false,"start_time":"2024-04-08T23:31:47.53451","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_accuracy(fcn_model1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_accuracy(unet_model1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_accuracy(deeplab_model1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* ### Intersection-over-Union (IoU)","metadata":{"papermill":{"duration":1.392648,"end_time":"2024-04-08T23:31:51.790944","exception":false,"start_time":"2024-04-08T23:31:50.398296","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nimport pandas as pd  \nimport numpy as np\n\ndef evaluate_metrics(dataset, model, n_classes=13):\n    # create mask\n    true_masks, predicted_masks = [], []\n    for images, masks in dataset:\n        pred_masks = model.predict(images)\n        pred_masks = tf.expand_dims(tf.argmax(pred_masks, axis=-1), axis=-1)\n        true_masks.extend(masks)\n        predicted_masks.extend(pred_masks)\n    true_masks = np.array(true_masks)\n    predicted_masks = np.array(predicted_masks)\n    \n    # initialize\n    class_wise_metrics = {\n        \"TP\": np.zeros(n_classes),  # True Positives\n        \"FP\": np.zeros(n_classes),  # False Positives\n        \"FN\": np.zeros(n_classes)   # False Negatives\n    }\n\n    # calculate metrics per class\n    for c_id in range(n_classes):\n        true_positive = (predicted_masks == c_id) & (true_masks == c_id)\n        false_positive = (predicted_masks == c_id) & (true_masks != c_id)\n        false_negative = (predicted_masks != c_id) & (true_masks == c_id)\n\n        class_wise_metrics[\"TP\"][c_id] += np.sum(true_positive)\n        class_wise_metrics[\"FP\"][c_id] += np.sum(false_positive)\n        class_wise_metrics[\"FN\"][c_id] += np.sum(false_negative)\n        \n    # class-wise metrics\n    recall = np.round(class_wise_metrics[\"TP\"] / (class_wise_metrics[\"TP\"] + class_wise_metrics[\"FN\"] + 1e-6), 2)\n    precision = np.round(class_wise_metrics[\"TP\"] / (class_wise_metrics[\"TP\"] + class_wise_metrics[\"FP\"] + 1e-6), 2)\n    iou = np.round(class_wise_metrics[\"TP\"] / (class_wise_metrics[\"TP\"] + class_wise_metrics[\"FP\"] + class_wise_metrics[\"FN\"] + 1e-6), 2)\n\n    # overall metrics\n    overall_recall = np.mean(recall)\n    overall_precision = np.mean(precision)\n    overall_iou = np.mean(iou)\n\n    # package evaluations\n    evaluations = {\n        \"class_wise\": {\n            \"Recall\": recall.tolist(),\n            \"Precision\": precision.tolist(),\n            \"IoU\": iou.tolist()\n        },\n        \"overall\": {\n            \"Recall\": round(overall_recall, 2),\n            \"Precision\": round(overall_precision, 2),\n            \"IoU\": round(overall_iou, 2)\n        }\n    }\n\n    return evaluations\n\n\ndef show_evaluations(evaluations):\n    class_wise_data = evaluations['class_wise']\n    class_wise_df = pd.DataFrame(class_wise_data)\n    class_wise_df.index = [f\"Class {i+1}\" for i in range(len(class_wise_data['Recall']))]\n    \n    overall_data = evaluations['overall']\n    overall_df = pd.DataFrame(overall_data, index=['All Classes'])\n    \n    # Combine \n    combined_df = pd.concat([overall_df, class_wise_df]).reset_index()\n    combined_df.rename(columns={'index': 'Class'}, inplace=True)\n    \n    return combined_df","metadata":{"papermill":{"duration":1.425355,"end_time":"2024-04-08T23:31:54.631465","exception":false,"start_time":"2024-04-08T23:31:53.20611","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-15T01:23:11.011473Z","iopub.execute_input":"2024-04-15T01:23:11.011825Z","iopub.status.idle":"2024-04-15T01:23:11.022979Z","shell.execute_reply.started":"2024-04-15T01:23:11.011798Z","shell.execute_reply":"2024-04-15T01:23:11.022016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***fcn***","metadata":{}},{"cell_type":"code","source":"evaluations = evaluate_metrics(train_dataset, fcn_model1)\nshow_evaluations(evaluations)","metadata":{"papermill":{"duration":135.246301,"end_time":"2024-04-08T23:34:11.31679","exception":false,"start_time":"2024-04-08T23:31:56.070489","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-15T01:23:14.770481Z","iopub.execute_input":"2024-04-15T01:23:14.770814Z","iopub.status.idle":"2024-04-15T01:26:02.27962Z","shell.execute_reply.started":"2024-04-15T01:23:14.77079Z","shell.execute_reply":"2024-04-15T01:26:02.278661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluations = evaluate_metrics(val_dataset, fcn_model1)\nshow_evaluations(evaluations)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluations = evaluate_metrics(test_dataset, fcn_model1)\nshow_evaluations(evaluations)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***unet***","metadata":{}},{"cell_type":"code","source":"evaluations = evaluate_metrics(train_dataset, unet_model1)\nshow_evaluations(evaluations)","metadata":{"papermill":{"duration":1.417603,"end_time":"2024-04-08T23:34:14.190389","exception":false,"start_time":"2024-04-08T23:34:12.772786","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluations = evaluate_metrics(val_dataset, unet_model1)\nshow_evaluations(evaluations)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluations = evaluate_metrics(test_dataset, unet_model1)\nshow_evaluations(evaluations)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***deeplab***","metadata":{}},{"cell_type":"code","source":"evaluations = evaluate_metrics(train_dataset, deeplab_model1)\nshow_evaluations(evaluations)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluations = evaluate_metrics(val_dataset, deeplab_model1)\nshow_evaluations(evaluations)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluations = evaluate_metrics(test_dataset, deeplab_model1)\nshow_evaluations(evaluations)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Prediction","metadata":{"papermill":{"duration":1.472772,"end_time":"2024-04-08T23:34:17.141109","exception":false,"start_time":"2024-04-08T23:34:15.668337","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"* ### Predict on Train, Val, Test Dataset","metadata":{}},{"cell_type":"code","source":"def create_mask(pred_mask):\n    pred_mask = tf.argmax(pred_mask, axis=-1)\n    pred_mask = pred_mask[..., tf.newaxis]\n    return pred_mask[0]","metadata":{"papermill":{"duration":1.45324,"end_time":"2024-04-08T23:34:33.470721","exception":false,"start_time":"2024-04-08T23:34:32.017481","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display(display_list):\n    plt.figure(figsize=(15, 15))\n\n    title = ['Input Image', 'True Mask', 'Predicted Mask']\n\n    for i in range(len(display_list)):\n        plt.subplot(1, len(display_list), i+1)\n        plt.title(title[i])\n        plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n        plt.axis('off')\n    plt.show()","metadata":{"papermill":{"duration":1.41042,"end_time":"2024-04-08T23:34:39.142061","exception":false,"start_time":"2024-04-08T23:34:37.731641","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_predictions(dataset, num, model):\n    \"\"\"\n    Displays the first image of each of the num batches\n    \"\"\"\n    if dataset:\n        for image, mask in dataset.take(num):\n            pred_mask = model.predict(image)\n            display([image[0], mask[0], create_mask(pred_mask)])\n    else:\n        display([sample_image, sample_mask,\n             create_mask(model.predict(sample_image[tf.newaxis, ...]))])","metadata":{"papermill":{"duration":1.419936,"end_time":"2024-04-08T23:34:44.812655","exception":false,"start_time":"2024-04-08T23:34:43.392719","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fcn\nshow_predictions(train_dataset, 3, fcn_model1)\nshow_predictions(val_dataset, 3, fcn_model1)\nshow_predictions(test_dataset, 3, fcn_model1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# unet\nshow_predictions(train_dataset, 3, unet_model1)\nshow_predictions(val_dataset, 3, unet_model1)\nshow_predictions(test_dataset, 3, unet_model1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# deeplab\nshow_predictions(train_dataset, 3, deeplab_model1)\nshow_predictions(val_dataset, 3, deeplab_model1)\nshow_predictions(test_dataset, 3, deeplab_model1)","metadata":{"papermill":{"duration":7.545865,"end_time":"2024-04-08T23:34:56.657157","exception":false,"start_time":"2024-04-08T23:34:49.111292","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* ### Predict on New Dataset","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\n\ndef load_and_preprocess_image(image_path, mask_path):\n    image = tf.io.read_file(image_path)\n    image = tf.image.decode_png(image, channels=3)\n    image = tf.image.resize(image, [256, 256], method='nearest')  \n    image = tf.image.convert_image_dtype(image, tf.float32)\n\n    mask = tf.io.read_file(mask_path)\n    mask = tf.image.decode_png(mask, channels=3)\n    mask = tf.image.resize(mask, [256, 256], method='nearest')  \n    mask = tf.math.reduce_max(mask, axis=-1, keepdims=True)\n\n    return image, mask\n\n# Paths to the new dataset\nIMAGE_PATH = Path(\"/kaggle/input/semantic-segmentation-car-driving/x-rgb\")\nMASK_PATH = Path(\"/kaggle/input/semantic-segmentation-car-driving/y-mask\")\nnew_images_paths = sorted([str(x) for x in IMAGE_PATH.glob(\"*.png\")])\nnew_masks_paths = sorted([str(x) for x in MASK_PATH.glob(\"*.png\")])\n\n# Creating the new dataset\nnew_dataset = tf.data.Dataset.from_tensor_slices((new_images_paths, new_masks_paths))\nnew_dataset = new_dataset.map(load_and_preprocess_image)\nnew_dataset_batched = new_dataset.batch(1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fcn\nshow_predictions(new_dataset_batched, 5, fcn_model1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# unet\nshow_predictions(new_dataset_batched, 5, unet_model1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#deeplab\nshow_predictions(new_dataset_batched, 5, deeplab_model1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Save Model","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\nfcn_model1.save('fcn1-image-segmentation-model.h5')\n\nunet_model1.save('unet1-image-segmentation-model.h5')\n\ndeeplab_model1.save('deeplab1-image-segmentation-model.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}